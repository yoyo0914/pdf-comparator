import os
import re
import io
import cv2
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import fitz  # PyMuPDF
from collections import defaultdict
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    import pytesseract
    TESSERACT_AVAILABLE = True
    # è¨­å®šTesseractè·¯å¾‘ (WSL/Ubuntu)
    pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'
except ImportError:
    TESSERACT_AVAILABLE = False

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

class FinancialTableAgent:
    """å°ˆæ¥­è²¡å ±è¡¨æ ¼è™•ç†AI Agent"""
    
    def __init__(self):
        self.table_patterns = {
            # æŠ•è³‡æ˜ç´°è¡¨æ ¼æ¨¡å¼
            'investment_table': {
                'keywords': ['æŠ•è³‡', 'æŒæœ‰', 'è­‰åˆ¸', 'å…¬å¸', 'è‚¡æ•¸', 'é‡‘é¡', 'å…¬å…åƒ¹å€¼'],
                'structure_indicators': ['TSMC', 'USD', '%', 'è‚¡', 'ä»Ÿå…ƒ', 'è¬å…ƒ']
            },
            # æç›Šè¡¨æ¨¡å¼
            'income_statement': {
                'keywords': ['ç‡Ÿæ¥­æ”¶å…¥', 'ç‡Ÿæ¥­æˆæœ¬', 'æ¯›åˆ©', 'æ·¨åˆ©', 'ç¨…å‰'],
                'structure_indicators': ['åƒå…ƒ', 'å¹´åº¦', 'æœ¬æœŸ', 'å»å¹´åŒæœŸ']
            },
            # è³‡ç”¢è² å‚µè¡¨æ¨¡å¼
            'balance_sheet': {
                'keywords': ['è³‡ç”¢', 'è² å‚µ', 'æ¬Šç›Š', 'æµå‹•', 'éæµå‹•'],
                'structure_indicators': ['ç¸½è¨ˆ', 'åˆè¨ˆ', 'å°è¨ˆ']
            },
            # ç¾é‡‘æµé‡è¡¨æ¨¡å¼
            'cash_flow': {
                'keywords': ['ç¾é‡‘æµé‡', 'ç‡Ÿæ¥­æ´»å‹•', 'æŠ•è³‡æ´»å‹•', 'èè³‡æ´»å‹•'],
                'structure_indicators': ['æµå…¥', 'æµå‡º', 'æ·¨é¡']
            }
        }
        
        # OCRé…ç½®
        self.ocr_configs = {
            'high_accuracy': r'--oem 3 --psm 6 -l chi_tra+eng',
            'table_structure': r'--oem 3 --psm 4 -l chi_tra+eng',
            'dense_text': r'--oem 3 --psm 11 -l chi_tra+eng',
            'sparse_text': r'--oem 3 --psm 8 -l chi_tra+eng'
        }
    
    def extract_text_from_pdf(self, pdf_path, output_path=None):
        """AI Agentä¸»è¦è™•ç†æµç¨‹"""
        logger.info(f"ğŸ¤– è²¡å ±è¡¨æ ¼AI Agentå•Ÿå‹•: {os.path.basename(pdf_path)}")
        
        # æª¢æŸ¥å·¥å…·å¯ç”¨æ€§
        self._check_dependencies()
        
        try:
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            
            # Agentè™•ç†çµ±è¨ˆ
            agent_stats = {
                'total_pages': total_pages,
                'table_pages': 0,
                'ocr_pages': 0,
                'hybrid_pages': 0,
                'failed_pages': 0,
                'financial_tables_found': 0
            }
            
            extracted_content = []
            financial_data = defaultdict(list)
            
            print(f"ğŸ“Š é–‹å§‹è™•ç† {total_pages} é è²¡å ±")
            
            for page_num in range(total_pages):
                page = doc[page_num]
                
                # AIé é¢æ™ºèƒ½åˆ†æ
                page_analysis = self._ai_analyze_page(page, page_num)
                
                # æ ¹æ“šåˆ†æçµæœé¸æ“‡æœ€ä½³è™•ç†ç­–ç•¥
                processing_result = self._process_page_with_ai(page, page_num, page_analysis)
                
                # æ›´æ–°çµ±è¨ˆ
                agent_stats[f"{processing_result['method']}_pages"] += 1
                if processing_result['is_financial_table']:
                    agent_stats['financial_tables_found'] += 1
                
                # æå–è²¡å‹™æ•¸æ“š
                if processing_result['content']:
                    extracted_financial = self._extract_financial_data(
                        processing_result['content'], page_num + 1
                    )
                    if extracted_financial:
                        for category, data in extracted_financial.items():
                            financial_data[category].extend(data)
                
                # æ ¼å¼åŒ–é é¢å…§å®¹
                formatted_page = self._format_agent_page(
                    processing_result, page_num + 1, page_analysis
                )
                extracted_content.append(formatted_page)
                
                # é€²åº¦é¡¯ç¤º
                if (page_num + 1) % 10 == 0:
                    print(f"âœ… å·²è™•ç† {page_num + 1}/{total_pages} é ")
            
            doc.close()
            
            # AI Agentç”Ÿæˆæœ€çµ‚å ±å‘Š
            final_report = self._generate_agent_report(
                extracted_content, financial_data, agent_stats
            )
            
            if output_path:
                self._save_agent_report(final_report, output_path)
            
            print(f"ğŸ‰ AI Agentè™•ç†å®Œæˆï¼")
            self._print_agent_summary(agent_stats)
            
            return final_report
            
        except Exception as e:
            logger.error(f"AI Agentè™•ç†å¤±æ•—: {e}")
            raise Exception(f"è²¡å ±AI AgentéŒ¯èª¤: {e}")
    
    def _check_dependencies(self):
        """æª¢æŸ¥ä¾è³´å·¥å…·"""
        tools_status = []
        
        if TESSERACT_AVAILABLE:
            try:
                # æ¸¬è©¦Tesseract
                test_result = pytesseract.get_tesseract_version()
                tools_status.append(f"âœ… Tesseract {test_result}")
            except Exception:
                tools_status.append("âŒ Tesseracté…ç½®éŒ¯èª¤")
        else:
            tools_status.append("âŒ Tesseractæœªå®‰è£")
        
        if PDFPLUMBER_AVAILABLE:
            tools_status.append("âœ… pdfplumberå¯ç”¨")
        else:
            tools_status.append("âŒ pdfplumberæœªå®‰è£")
        
        tools_status.append("âœ… PyMuPDFå¯ç”¨")
        tools_status.append("âœ… OpenCVå¯ç”¨")
        
        print("ğŸ”§ AI Agentå·¥å…·æª¢æŸ¥:")
        for status in tools_status:
            print(f"   {status}")
        print()
    
    def _ai_analyze_page(self, page, page_num):
        """AIæ™ºèƒ½é é¢åˆ†æ"""
        try:
            # å¤šé‡åˆ†æç­–ç•¥
            text_analysis = self._analyze_text_content(page)
            structure_analysis = self._analyze_page_structure(page)
            visual_analysis = self._analyze_visual_features(page)
            
            # AIæ±ºç­–é‚è¼¯
            analysis_result = {
                'page_num': page_num + 1,
                'content_type': self._determine_content_type(text_analysis, structure_analysis),
                'complexity_level': self._assess_complexity(text_analysis, structure_analysis, visual_analysis),
                'table_type': self._identify_table_type(text_analysis),
                'recommended_strategy': None,
                'confidence': 0.0
            }
            
            # AIæ¨è–¦è™•ç†ç­–ç•¥
            analysis_result['recommended_strategy'] = self._ai_recommend_strategy(analysis_result)
            
            return analysis_result
            
        except Exception as e:
            logger.warning(f"é é¢åˆ†æå¤±æ•— {page_num + 1}: {e}")
            return {
                'page_num': page_num + 1,
                'content_type': 'unknown',
                'complexity_level': 'medium',
                'recommended_strategy': 'fallback'
            }
    
    def _analyze_text_content(self, page):
        """åˆ†ææ–‡å­—å…§å®¹"""
        text = page.get_text()
        
        # è¨ˆç®—å„ç¨®ç‰¹å¾µ
        char_count = len(text)
        line_count = len(text.split('\n'))
        
        # è²¡å‹™é—œéµè©å¯†åº¦
        financial_keywords = 0
        for pattern_name, pattern_info in self.table_patterns.items():
            for keyword in pattern_info['keywords']:
                financial_keywords += text.lower().count(keyword.lower())
        
        # æ•¸å­—å¯†åº¦
        numbers = re.findall(r'\d{1,3}(?:,\d{3})*(?:\.\d+)?', text)
        number_density = len(numbers) / max(char_count, 1) * 1000
        
        # çµæ§‹æŒ‡æ¨™
        structure_indicators = 0
        for pattern_name, pattern_info in self.table_patterns.items():
            for indicator in pattern_info['structure_indicators']:
                structure_indicators += text.count(indicator)
        
        return {
            'char_count': char_count,
            'line_count': line_count,
            'financial_keywords': financial_keywords,
            'number_density': number_density,
            'structure_indicators': structure_indicators,
            'has_substantial_content': char_count > 100
        }
    
    def _analyze_page_structure(self, page):
        """åˆ†æé é¢çµæ§‹"""
        try:
            # ç²å–æ–‡å­—å¡Šä¿¡æ¯
            text_dict = page.get_text("dict")
            
            blocks_count = len(text_dict.get("blocks", []))
            
            # åˆ†æå­—é«”å’Œä½ç½®åˆ†ä½ˆ
            font_sizes = []
            positions = []
            
            for block in text_dict.get("blocks", []):
                if "lines" in block:
                    for line in block["lines"]:
                        for span in line["spans"]:
                            font_sizes.append(span.get("size", 12))
                            positions.append(span["bbox"])
            
            # è¨ˆç®—çµæ§‹ç‰¹å¾µ
            font_variance = np.var(font_sizes) if font_sizes else 0
            
            return {
                'blocks_count': blocks_count,
                'font_variance': font_variance,
                'has_structured_layout': blocks_count > 5 and font_variance > 2,
                'has_table_structure': self._detect_table_structure(text_dict)
            }
            
        except Exception:
            return {
                'blocks_count': 0,
                'has_structured_layout': False,
                'has_table_structure': False
            }
    
    def _detect_table_structure(self, text_dict):
        """æª¢æ¸¬è¡¨æ ¼çµæ§‹"""
        # åˆ†ææ–‡å­—å¡Šçš„å°é½Šæ¨¡å¼
        aligned_blocks = 0
        total_blocks = 0
        
        for block in text_dict.get("blocks", []):
            if "lines" in block:
                total_blocks += 1
                # æª¢æŸ¥æ˜¯å¦æœ‰è¦å¾‹çš„ä½ç½®æ’åˆ—
                line_positions = []
                for line in block["lines"]:
                    if line["spans"]:
                        line_positions.append(line["spans"][0]["bbox"][0])
                
                # å¦‚æœä½ç½®ç›¸å°è¦å¾‹ï¼Œèªç‚ºæ˜¯å°é½Šçš„
                if len(set(line_positions)) < len(line_positions) * 0.8:
                    aligned_blocks += 1
        
        return aligned_blocks / max(total_blocks, 1) > 0.3
    
    def _analyze_visual_features(self, page):
        """åˆ†æè¦–è¦ºç‰¹å¾µ"""
        try:
            # å°‡é é¢è½‰ç‚ºåœ–åƒé€²è¡Œè¦–è¦ºåˆ†æ
            mat = fitz.Matrix(1.5, 1.5)  # ä¸­ç­‰è§£æåº¦
            pix = page.get_pixmap(matrix=mat)
            img_data = pix.tobytes("png")
            
            # è¼‰å…¥åœ–åƒ
            image = Image.open(io.BytesIO(img_data))
            
            # è½‰ç‚ºç°éšåˆ†æ
            gray_image = image.convert('L')
            img_array = np.array(gray_image)
            
            # æª¢æ¸¬ç·šæ¢ï¼ˆè¡¨æ ¼é‚Šæ¡†ï¼‰
            edges = cv2.Canny(img_array, 50, 150)
            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, minLineLength=100, maxLineGap=10)
            
            line_count = len(lines) if lines is not None else 0
            
            # æª¢æ¸¬æ–‡å­—å€åŸŸå¯†åº¦
            text_density = np.sum(img_array < 200) / img_array.size
            
            return {
                'has_lines': line_count > 10,
                'line_count': line_count,
                'text_density': text_density,
                'is_mostly_text': text_density > 0.1
            }
            
        except Exception:
            return {
                'has_lines': False,
                'line_count': 0,
                'text_density': 0,
                'is_mostly_text': False
            }
    
    def _determine_content_type(self, text_analysis, structure_analysis):
        """ç¢ºå®šå…§å®¹é¡å‹"""
        if text_analysis['financial_keywords'] > 5 and structure_analysis['has_table_structure']:
            return 'complex_financial_table'
        elif text_analysis['financial_keywords'] > 2:
            return 'financial_content'
        elif structure_analysis['has_structured_layout']:
            return 'structured_text'
        elif text_analysis['has_substantial_content']:
            return 'plain_text'
        else:
            return 'minimal_content'
    
    def _assess_complexity(self, text_analysis, structure_analysis, visual_analysis):
        """è©•ä¼°è¤‡é›œåº¦"""
        complexity_score = 0
        
        # æ–‡å­—è¤‡é›œåº¦
        if text_analysis['number_density'] > 10:
            complexity_score += 2
        if text_analysis['financial_keywords'] > 5:
            complexity_score += 2
        
        # çµæ§‹è¤‡é›œåº¦
        if structure_analysis['has_table_structure']:
            complexity_score += 2
        if structure_analysis['font_variance'] > 5:
            complexity_score += 1
        
        # è¦–è¦ºè¤‡é›œåº¦
        if visual_analysis['line_count'] > 20:
            complexity_score += 2
        
        if complexity_score >= 6:
            return 'high'
        elif complexity_score >= 3:
            return 'medium'
        else:
            return 'low'
    
    def _identify_table_type(self, text_analysis):
        """è­˜åˆ¥è¡¨æ ¼é¡å‹"""
        text = str(text_analysis)  # ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›æ‡‰è©²å‚³å…¥åŸå§‹æ–‡å­—
        
        for table_type, pattern_info in self.table_patterns.items():
            score = 0
            for keyword in pattern_info['keywords']:
                if keyword in text.lower():
                    score += 1
            
            if score >= 2:
                return table_type
        
        return 'unknown'
    
    def _ai_recommend_strategy(self, analysis_result):
        """AIæ¨è–¦è™•ç†ç­–ç•¥"""
        content_type = analysis_result['content_type']
        complexity = analysis_result['complexity_level']
        
        if content_type == 'complex_financial_table':
            if TESSERACT_AVAILABLE:
                return 'ocr_enhanced'
            else:
                return 'hybrid'
        elif content_type == 'financial_content':
            if complexity == 'high':
                return 'hybrid'
            else:
                return 'text_extraction'
        elif content_type == 'structured_text':
            return 'structured_extraction'
        else:
            return 'basic_extraction'
    
    def _process_page_with_ai(self, page, page_num, analysis):
        """AIæ™ºèƒ½è™•ç†é é¢"""
        strategy = analysis['recommended_strategy']
        
        try:
            if strategy == 'ocr_enhanced':
                content = self._process_with_ocr_enhanced(page)
                method = 'ocr'
            elif strategy == 'hybrid':
                content = self._process_with_hybrid_method(page)
                method = 'hybrid'
            elif strategy == 'structured_extraction':
                content = self._process_with_structured_extraction(page)
                method = 'table'
            else:
                content = self._process_with_basic_extraction(page)
                method = 'table'
            
            return {
                'content': content,
                'method': method,
                'strategy': strategy,
                'is_financial_table': analysis['content_type'] == 'complex_financial_table',
                'success': bool(content and len(content) > 50)
            }
            
        except Exception as e:
            logger.warning(f"è™•ç†é é¢ {page_num + 1} å¤±æ•—: {e}")
            return {
                'content': f"ç¬¬ {page_num + 1} é è™•ç†å¤±æ•—: {e}",
                'method': 'failed',
                'strategy': 'fallback',
                'is_financial_table': False,
                'success': False
            }
    
    def _process_with_ocr_enhanced(self, page):
        """OCRå¢å¼·è™•ç†"""
        if not TESSERACT_AVAILABLE:
            return self._process_with_basic_extraction(page)
        
        try:
            # é«˜è§£æåº¦åœ–åƒ
            mat = fitz.Matrix(3.0, 3.0)
            pix = page.get_pixmap(matrix=mat)
            img_data = pix.tobytes("png")
            
            # åœ–åƒå¢å¼·
            image = Image.open(io.BytesIO(img_data))
            enhanced_image = self._enhance_for_table_ocr(image)
            
            # å¤šæ¬¡OCRå˜—è©¦
            ocr_results = []
            
            for config_name, config in self.ocr_configs.items():
                try:
                    result = pytesseract.image_to_string(
                        enhanced_image, 
                        config=config,
                        lang='chi_tra+eng'
                    )
                    if result and len(result.strip()) > 100:
                        ocr_results.append((config_name, result))
                except Exception:
                    continue
            
            # é¸æ“‡æœ€ä½³çµæœ
            if ocr_results:
                best_result = max(ocr_results, key=lambda x: len(x[1]))
                processed_text = self._post_process_ocr_result(best_result[1])
                return self._reconstruct_table_from_ocr(processed_text)
            
            return self._process_with_basic_extraction(page)
            
        except Exception as e:
            logger.warning(f"OCRå¢å¼·è™•ç†å¤±æ•—: {e}")
            return self._process_with_basic_extraction(page)
    
    def _enhance_for_table_ocr(self, image):
        """ç‚ºè¡¨æ ¼OCRå¢å¼·åœ–åƒ"""
        try:
            # è½‰æ›ç‚ºOpenCVæ ¼å¼
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # è½‰ç‚ºç°éš
            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
            
            # å»å™ª
            denoised = cv2.fastNlMeansDenoising(gray)
            
            # å¢å¼·å°æ¯”åº¦
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
            enhanced = clahe.apply(denoised)
            
            # éŠ³åŒ–
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(enhanced, -1, kernel)
            
            # è‡ªé©æ‡‰äºŒå€¼åŒ–
            binary = cv2.adaptiveThreshold(sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            
            return Image.fromarray(binary)
            
        except Exception:
            return image
    
    def _post_process_ocr_result(self, text):
        """å¾Œè™•ç†OCRçµæœ"""
        if not text:
            return ""
        
        # OCRéŒ¯èª¤ä¿®æ­£å­—å…¸
        corrections = {
            # æ•¸å­—ä¿®æ­£
            'o': '0', 'O': '0', 'â—‹': '0',
            'l': '1', 'I': '1', '|': '1',
            'S': '5', 's': '5',
            'G': '6', 'g': '6',
            'B': '8',
            # æ¨™é»ä¿®æ­£
            'ï¼Œ': ',', 'ã€‚': '.', 'ï¼…': '%',
            'ï¼ˆ': '(', 'ï¼‰': ')',
            # å¸¸è¦‹è©å½™ä¿®æ­£
            'å°ç©éœ': 'å°ç©é›»',
            'TSMC.*?lobal': 'TSMC Global',
        }
        
        # æ‡‰ç”¨ä¿®æ­£
        corrected_text = text
        for wrong, correct in corrections.items():
            if len(wrong) == 1:  # å–®å­—ç¬¦ä¿®æ­£
                # åœ¨æ•¸å­—ç’°å¢ƒä¸­ä¿®æ­£
                corrected_text = re.sub(f'(\\d){re.escape(wrong)}(\\d)', f'\\1{correct}\\2', corrected_text)
            else:  # çŸ­èªä¿®æ­£
                corrected_text = re.sub(wrong, correct, corrected_text, flags=re.IGNORECASE)
        
        return corrected_text
    
    def _reconstruct_table_from_ocr(self, text):
        """å¾OCRçµæœé‡å»ºè¡¨æ ¼"""
        if not text:
            return ""
        
        lines = text.split('\n')
        reconstructed_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºè¡¨æ ¼è¡Œ
            if self._is_table_row(line):
                formatted_line = self._format_table_row(line)
                reconstructed_lines.append(formatted_line)
            else:
                reconstructed_lines.append(line)
        
        return '\n'.join(reconstructed_lines)
    
    def _is_table_row(self, line):
        """åˆ¤æ–·æ˜¯å¦ç‚ºè¡¨æ ¼è¡Œ"""
        # åŒ…å«æ•¸å­—å’Œè¡¨æ ¼ç‰¹å¾µ
        has_numbers = bool(re.search(r'\d', line))
        has_separators = bool(re.search(r'[|\t]|  {2,}', line))
        has_financial_indicators = bool(re.search(r'USD|TWD|NT\$|%|è‚¡|è¬|å„„', line))
        
        return has_numbers and (has_separators or has_financial_indicators)
    
    def _format_table_row(self, line):
        """æ ¼å¼åŒ–è¡¨æ ¼è¡Œ"""
        # æ™ºèƒ½åˆ†å‰²å’Œå°é½Š
        parts = re.split(r'  {2,}|\t+', line)
        if len(parts) > 1:
            cleaned_parts = [part.strip() for part in parts if part.strip()]
            return ' | '.join(cleaned_parts)
        
        return line
    
    def _process_with_hybrid_method(self, page):
        """æ··åˆæ–¹æ³•è™•ç†"""
        results = []
        
        # æ–¹æ³•1: åŸºæœ¬æ–‡å­—æå–
        basic_text = page.get_text()
        if basic_text:
            results.append("=== åŸºæœ¬æ–‡å­—æå– ===")
            results.append(self._clean_basic_text(basic_text))
        
        # æ–¹æ³•2: çµæ§‹åŒ–æå–
        try:
            structured_text = self._extract_structured_layout(page)
            if structured_text and len(structured_text) > 100:
                results.append("=== çµæ§‹åŒ–æå– ===")
                results.append(structured_text)
        except Exception:
            pass
        
        # æ–¹æ³•3: OCRè£œå……ï¼ˆå¦‚æœå¯ç”¨ä¸”å‰é¢çµæœä¸è¶³ï¼‰
        if TESSERACT_AVAILABLE and sum(len(r) for r in results) < 500:
            try:
                ocr_text = self._simple_ocr_extract(page)
                if ocr_text and len(ocr_text) > 100:
                    results.append("=== OCRè£œå…… ===")
                    results.append(ocr_text)
            except Exception:
                pass
        
        return '\n\n'.join(results) if results else "æ··åˆè™•ç†å¤±æ•—"
    
    def _extract_structured_layout(self, page):
        """æå–çµæ§‹åŒ–ä½ˆå±€"""
        try:
            text_dict = page.get_text("dict")
            
            # æŒ‰ä½ç½®é‡çµ„æ–‡å­—
            elements = []
            for block in text_dict.get("blocks", []):
                if "lines" not in block:
                    continue
                
                for line in block["lines"]:
                    line_elements = []
                    for span in line["spans"]:
                        if span["text"].strip():
                            line_elements.append({
                                "text": span["text"].strip(),
                                "x": span["bbox"][0],
                                "y": span["bbox"][1]
                            })
                    
                    if line_elements:
                        # æŒ‰Xåº§æ¨™æ’åº
                        line_elements.sort(key=lambda x: x["x"])
                        elements.append({
                            "y": line_elements[0]["y"],
                            "elements": line_elements
                        })
            
            # æŒ‰Yåº§æ¨™æ’åº
            elements.sort(key=lambda x: x["y"])
            
            # é‡å»ºçµæ§‹
            structured_lines = []
            for line_data in elements:
                line_text = self._format_line_with_spacing(line_data["elements"])
                if line_text.strip():
                    structured_lines.append(line_text)
            
            return '\n'.join(structured_lines)
            
        except Exception:
            return ""
    
    def _format_line_with_spacing(self, elements):
        """æ ¹æ“šä½ç½®æ ¼å¼åŒ–è¡Œ"""
        if not elements:
            return ""
        
        formatted_parts = []
        last_x = 0
        
        for elem in elements:
            current_x = elem["x"]
            gap = current_x - last_x
            
            if last_x > 0:
                if gap > 120:
                    formatted_parts.append(" | ")
                elif gap > 60:
                    formatted_parts.append(" | ")
                elif gap > 20:
                    formatted_parts.append("  ")
                elif gap > 8:
                    formatted_parts.append(" ")
            
            formatted_parts.append(elem["text"])
            last_x = current_x + len(elem["text"]) * 6
        
        return "".join(formatted_parts)
    
    def _simple_ocr_extract(self, page):
        """ç°¡å–®OCRæå–"""
        try:
            mat = fitz.Matrix(2.0, 2.0)
            pix = page.get_pixmap(matrix=mat)
            img_data = pix.tobytes("png")
            
            image = Image.open(io.BytesIO(img_data))
            
            result = pytesseract.image_to_string(
                image,
                config=self.ocr_configs['high_accuracy'],
                lang='chi_tra+eng'
            )
            
            return self._post_process_ocr_result(result)
            
        except Exception:
            return ""
    
    def _process_with_structured_extraction(self, page):
        """çµæ§‹åŒ–æå–è™•ç†"""
        return self._extract_structured_layout(page)
    
    def _process_with_basic_extraction(self, page):
        """åŸºæœ¬æå–è™•ç†"""
        return self._clean_basic_text(page.get_text())
    
    def _clean_basic_text(self, text):
        """æ¸…ç†åŸºæœ¬æ–‡å­—"""
        if not text:
            return ""
        
        # ä¿®å¾©å¸¸è¦‹PDFå•é¡Œ
        text = re.sub(r'(\d)\s+([,ï¼Œ])\s*(\d)', r'\1\2\3', text)
        text = re.sub(r'(\d)\s+([.])\s*(\d)', r'\1\2\3', text)
        text = re.sub(r'(\d)\s*([%ï¼…])', r'\1\2', text)
        text = re.sub(r'([\$ï¼„])\s+(\d)', r'\1\2', text)
        
        # æ¸…ç†ç©ºè¡Œ
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        
        return text.strip()
    
    def _extract_financial_data(self, content, page_num):
        """å¾å…§å®¹ä¸­æå–è²¡å‹™æ•¸æ“š"""
        financial_data = defaultdict(list)
        
        if not content:
            return financial_data
        
        # è²¡å‹™æ•¸æ“šæ¨¡å¼
        patterns = {
            'revenue': [
                r'ç‡Ÿæ¥­æ”¶å…¥[æ·¨]?[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)',
                r'revenue[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)',
                r'æ·¨ç‡Ÿæ¥­æ”¶å…¥[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)'
            ],
            'net_income': [
                r'æ·¨åˆ©[æ½¤]?[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)',
                r'æœ¬æœŸæ·¨åˆ©[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)',
                r'net\s+income[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)'
            ],
            'total_assets': [
                r'è³‡ç”¢ç¸½é¡[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)',
                r'ç¸½è³‡ç”¢[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)',
                r'total\s+assets[ï¼š:\s]*[NT\$]?\s*(\d{1,3}(?:,\d{3})*)'
            ]
        }
        
        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.finditer(pattern, content, re.IGNORECASE)
                for match in matches:
                    try:
                        value_str = match.group(1).replace(',', '')
                        value = float(value_str)
                        financial_data[category].append({
                            'value': value,
                            'page': page_num,
                            'context': match.group(0)
                        })
                    except ValueError:
                        continue
        
        return financial_data
    
    def _format_agent_page(self, processing_result, page_num, analysis):
        """æ ¼å¼åŒ–Agenté é¢è¼¸å‡º"""
        header_parts = [
            f"ç¬¬ {page_num} é ",
            f"[{analysis['content_type']}]",
            f"è¤‡é›œåº¦: {analysis['complexity_level']}",
            f"ç­–ç•¥: {processing_result['strategy']}"
        ]
        
        if processing_result['is_financial_table']:
            header_parts.append("ğŸ¦ è²¡å‹™è¡¨æ ¼")
        
        if processing_result['success']:
            header_parts.append("âœ…")
        else:
            header_parts.append("âš ï¸")
        
        header = " - ".join(header_parts)
        content = processing_result['content']
        
        return f"\n{'='*80}\n{header}\n{'='*80}\n{content}"
    
    def _generate_agent_report(self, extracted_content, financial_data, agent_stats):
        """ç”ŸæˆAI Agentå ±å‘Š"""
        report_parts = []
        
        # å ±å‘Šé ­éƒ¨
        report_parts.append("=" * 100)
        report_parts.append("è²¡å ±è¡¨æ ¼è™•ç†AI Agent - å®Œæ•´åˆ†æå ±å‘Š")
        report_parts.append("=" * 100)
        
        # Agentçµ±è¨ˆ
        report_parts.append(f"\nğŸ¤– AI Agentè™•ç†çµ±è¨ˆ:")
        report_parts.append(f"  ç¸½é æ•¸: {agent_stats['total_pages']}")
        report_parts.append(f"  è¡¨æ ¼é : {agent_stats['table_pages']}")
        report_parts.append(f"  OCRé : {agent_stats['ocr_pages']}")
        report_parts.append(f"  æ··åˆé : {agent_stats['hybrid_pages']}")
        report_parts.append(f"  å¤±æ•—é : {agent_stats['failed_pages']}")
        report_parts.append(f"  è²¡å‹™è¡¨æ ¼: {agent_stats['financial_tables_found']} å€‹")
        
        # å·¥å…·ä½¿ç”¨çµ±è¨ˆ
        success_rate = (agent_stats['total_pages'] - agent_stats['failed_pages']) / agent_stats['total_pages'] * 100
        report_parts.append(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        
        # è²¡å‹™æ•¸æ“šæ‘˜è¦
        if financial_data:
            report_parts.append(f"\nğŸ’° AIæå–çš„è²¡å‹™æ•¸æ“š:")
            for category, data_list in financial_data.items():
                if data_list:
                    unique_values = len(set(item['value'] for item in data_list))
                    report_parts.append(f"  {category}: {len(data_list)} å€‹æ•¸æ“šé» ({unique_values} å€‹å”¯ä¸€å€¼)")
        
        # å®Œæ•´å…§å®¹
        report_parts.append(f"\nğŸ“„ AI Agentå®Œæ•´è™•ç†çµæœ:")
        report_parts.extend(extracted_content)
        
        return '\n'.join(report_parts)
    
    def _save_agent_report(self, report, output_path):
        """å„²å­˜Agentå ±å‘Š"""
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"AI Agentå ±å‘Šå·²å„²å­˜: {output_path}")
            
        except Exception as e:
            logger.error(f"å„²å­˜å¤±æ•—: {e}")
    
    def _print_agent_summary(self, stats):
        """æ‰“å°Agentè™•ç†æ‘˜è¦"""
        print(f"\nAI Agentè™•ç†æ‘˜è¦:")
        print(f"æ™ºèƒ½ç­–ç•¥é¸æ“‡")
        print(f"æˆåŠŸè™•ç†: {stats['total_pages'] - stats['failed_pages']}/{stats['total_pages']} é ")
        print(f"è²¡å‹™è¡¨æ ¼ç™¼ç¾: {stats['financial_tables_found']} å€‹")
        print(f"OCRå¢å¼·: {stats['ocr_pages']} é ")
        print(f"æ··åˆè™•ç†: {stats['hybrid_pages']} é ")
        
        if TESSERACT_AVAILABLE:
            print(f"OCRå¼•æ“: å¯ç”¨")
        else:
            print(f"OCRå¼•æ“: ä¸å¯ç”¨ (å»ºè­°å®‰è£)")
    
    def process_reports(self, report_a_path, report_b_path, output_dir="outputs"):
        """AI Agentè™•ç†å…©ä»½å ±å‘Š"""
        results = {}
        
        print("ğŸ¤– è²¡å ±è¡¨æ ¼è™•ç†AI Agentå•Ÿå‹•")
        print("=" * 60)
        
        try:
            # è™•ç†å ±å‘ŠA
            print(f"\nğŸ“Š AI Agentè™•ç†è²¡å ±A: {os.path.basename(report_a_path)}")
            output_a = os.path.join(output_dir, "report_a_agent.txt")
            text_a = self.extract_text_from_pdf(report_a_path, output_a)
            results['report_a'] = text_a
            
            # è™•ç†å ±å‘ŠB
            print(f"\nğŸ“Š AI Agentè™•ç†è²¡å ±B: {os.path.basename(report_b_path)}")
            output_b = os.path.join(output_dir, "report_b_agent.txt")
            text_b = self.extract_text_from_pdf(report_b_path, output_b)
            results['report_b'] = text_b
            
            print(f"\nğŸ‰ AI Agentä»»å‹™å®Œæˆï¼")
            print(f"   çµæœæ–‡ä»¶: {output_dir}/report_*_agent.txt")
            
            return results
            
        except Exception as e:
            logger.error(f"AI Agentä»»å‹™å¤±æ•—: {e}")
            raise


# å‘å¾Œå…¼å®¹
class PDFParser(FinancialTableAgent):
    """å‘å¾Œå…¼å®¹çš„é¡å"""
    pass