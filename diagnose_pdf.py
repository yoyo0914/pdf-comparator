"""
PDFæå–å“è³ªè¨ºæ–·å·¥å…·
"""
import re
import os

def diagnose_pdf_extraction_quality():
    """è¨ºæ–·PDFæå–å“è³ª"""
    
    report_files = {
        'report_a': 'outputs/report_a_text.txt',
        'report_b': 'outputs/report_b_text.txt'
    }
    
    print("ğŸ” PDFæå–å“è³ªè¨ºæ–·å ±å‘Š")
    print("=" * 60)
    
    for report_name, file_path in report_files.items():
        if not os.path.exists(file_path):
            print(f"âŒ {report_name}: æ–‡ä»¶ä¸å­˜åœ¨")
            continue
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"\nğŸ“„ {report_name.upper()} è¨ºæ–·çµæœ:")
        print("-" * 40)
        
        # åŸºæœ¬çµ±è¨ˆ
        char_count = len(content)
        line_count = len(content.split('\n'))
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {char_count:,} å­—ç¬¦, {line_count:,} è¡Œ")
        
        # å¹´åº¦è­˜åˆ¥æª¢æŸ¥
        years_mentioned = diagnose_year_identification(content)
        print(f"ğŸ“… å¹´åº¦è­˜åˆ¥: {years_mentioned}")
        
        # è²¡å‹™æ•¸æ“šå“è³ª
        financial_data_quality = diagnose_financial_data(content)
        print(f"ğŸ’° è²¡å‹™æ•¸æ“šå“è³ª: {financial_data_quality}")
        
        # è¡¨æ ¼çµæ§‹æª¢æŸ¥
        table_quality = diagnose_table_structure(content)
        print(f"ğŸ“‹ è¡¨æ ¼çµæ§‹: {table_quality}")
        
        # é é¢åˆ†é¡çµ±è¨ˆ
        page_classification = diagnose_page_classification(content)
        print(f"ğŸ“‘ é é¢åˆ†é¡: {page_classification}")
        
        # å…§å®¹ç¤ºä¾‹ï¼ˆå‰1000å­—ç¬¦ï¼‰
        print(f"ğŸ“ å…§å®¹ç¤ºä¾‹:")
        clean_sample = content[:1000].replace('\n\n\n', '\n').strip()
        print(f"```\n{clean_sample}\n```")
        
        # è²¡å‹™é—œéµè©æª¢æŸ¥
        keywords_found = diagnose_keywords(content)
        print(f"ğŸ”‘ é—œéµè©æª¢æ¸¬: {keywords_found}")

def diagnose_year_identification(content):
    """è¨ºæ–·å¹´åº¦è­˜åˆ¥å•é¡Œ"""
    
    # å°‹æ‰¾å„ç¨®å¹´åº¦æ ¼å¼
    patterns = {
        'æ°‘åœ‹å¹´': r'æ°‘åœ‹\s*(\d{2,3})\s*å¹´',
        'è¥¿å…ƒå¹´': r'(20\d{2})\s*å¹´',
        'å¹´åº¦æ¨™ç¤º': r'(\d{2,3})\s*å¹´åº¦',
        'æœŸé–“æ¨™ç¤º': r'(\d{4})\s*å¹´\s*\d+\s*æœˆ'
    }
    
    results = {}
    for pattern_name, pattern in patterns.items():
        matches = re.findall(pattern, content)
        if matches:
            unique_years = list(set(matches))
            results[pattern_name] = unique_years[:5]  # æœ€å¤šé¡¯ç¤º5å€‹
    
    return results

def diagnose_financial_data(content):
    """è¨ºæ–·è²¡å‹™æ•¸æ“šå“è³ª"""
    
    # å°‹æ‰¾è²¡å‹™æ•¸æ“šæ¨¡å¼
    financial_patterns = {
        'å¤§é‡‘é¡': r'\$?\s*\d{1,3}(?:,\d{3}){2,}',  # ç™¾è¬ä»¥ä¸Šé‡‘é¡
        'ç‡Ÿæ”¶é—œéµè©': r'ç‡Ÿæ¥­æ”¶å…¥|revenue|sales',
        'ç²åˆ©é—œéµè©': r'æ·¨åˆ©|profit|net income',
        'è³‡ç”¢é—œéµè©': r'ç¸½è³‡ç”¢|total assets|è³‡ç”¢ç¸½é¡'
    }
    
    results = {}
    for category, pattern in financial_patterns.items():
        matches = re.findall(pattern, content, re.IGNORECASE)
        results[category] = len(matches)
    
    # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
    sample_numbers = re.findall(r'\$?\s*(\d{1,3}(?:,\d{3}){2,})', content)
    results['å¤§é‡‘é¡æ¨£æœ¬'] = sample_numbers[:3] if sample_numbers else ['ç„¡']
    
    return results

def diagnose_table_structure(content):
    """è¨ºæ–·è¡¨æ ¼çµæ§‹å“è³ª"""
    
    lines = content.split('\n')
    
    # çµ±è¨ˆè¡¨æ ¼ç‰¹å¾µ
    tab_lines = sum(1 for line in lines if '\t' in line)
    aligned_lines = sum(1 for line in lines if re.search(r'\s{4,}', line))
    table_headers = sum(1 for line in lines if re.search(r'é …ç›®|é‡‘é¡|å¹´åº¦|item|amount', line, re.IGNORECASE))
    
    # æª¢æŸ¥è¡¨æ ¼å“è³ª
    total_lines = len(lines)
    table_ratio = (tab_lines + aligned_lines) / total_lines if total_lines > 0 else 0
    
    quality_score = "å„ªç§€" if table_ratio > 0.3 else "è‰¯å¥½" if table_ratio > 0.1 else "å·®"
    
    return {
        'åˆ¶è¡¨ç¬¦è¡Œæ•¸': tab_lines,
        'å°é½Šè¡Œæ•¸': aligned_lines,
        'è¡¨æ ¼æ¨™é¡Œ': table_headers,
        'è¡¨æ ¼æ¯”ä¾‹': f"{table_ratio:.1%}",
        'å“è³ªè©•åˆ†': quality_score
    }

def diagnose_page_classification(content):
    """è¨ºæ–·é é¢åˆ†é¡"""
    
    # çµ±è¨ˆä¸åŒé¡å‹çš„é é¢æ¨™è¨˜
    page_markers = re.findall(r'ç¬¬\s*(\d+)\s*é \s*\(([^)]+)\)', content)
    
    if not page_markers:
        return "ç„¡é é¢åˆ†é¡æ¨™è¨˜"
    
    # çµ±è¨ˆé é¢é¡å‹
    page_types = {}
    for page_num, page_type in page_markers:
        page_types[page_type] = page_types.get(page_type, 0) + 1
    
    return page_types

def diagnose_keywords(content):
    """è¨ºæ–·é—œéµè©è¦†è“‹"""
    
    essential_keywords = {
        'å°ç©é›»': r'å°ç©é›»|TSMC|taiwan semiconductor',
        'ç‡Ÿæ¥­æ”¶å…¥': r'ç‡Ÿæ¥­æ”¶å…¥|revenue|sales',
        'æ·¨åˆ©æ½¤': r'æ·¨åˆ©|net income|profit',
        'ç¾é‡‘æµ': r'ç¾é‡‘æµ|cash flow',
        'è³‡ç”¢è² å‚µ': r'è³‡ç”¢|è² å‚µ|assets|liabilities',
        'æŠ•è³‡': r'æŠ•è³‡|investment',
        'ç ”ç™¼': r'ç ”ç™¼|research|development|R&D'
    }
    
    results = {}
    for keyword, pattern in essential_keywords.items():
        matches = len(re.findall(pattern, content, re.IGNORECASE))
        results[keyword] = matches
    
    return results

def generate_improvement_suggestions():
    """ç”Ÿæˆæ”¹é€²å»ºè­°"""
    
    print("\nğŸ’¡ æ”¹é€²å»ºè­°:")
    print("=" * 60)
    
    suggestions = [
        "1. æª¢æŸ¥PDFåŸå§‹å“è³ªï¼šæ˜¯å¦ç‚ºæƒæç‰ˆæˆ–åœ–ç‰‡PDF",
        "2. é‡æ–°æª¢è¦–è¡¨æ ¼æå–é‚è¼¯ï¼šæ•¸å­—å’Œæ¨™é¡Œçš„å°æ‡‰é—œä¿‚",
        "3. åŠ å¼·å¹´åº¦è­˜åˆ¥ï¼šçµ±ä¸€æ™‚é–“æ ¼å¼è™•ç†",
        "4. æ”¹é€²è²¡å‹™æ•¸æ“šåŒ¹é…ï¼šç¢ºä¿æ•¸æ“šå’Œç§‘ç›®æ­£ç¢ºå°æ‡‰",
        "5. å„ªåŒ–é é¢åˆ†é¡ï¼šä¸åŒé¡å‹é é¢ä½¿ç”¨ä¸åŒè§£æç­–ç•¥",
        "6. å¢åŠ æ•¸æ“šé©—è­‰ï¼šæª¢æŸ¥æå–æ•¸æ“šçš„åˆç†æ€§",
        "7. è€ƒæ…®ä½¿ç”¨OCRï¼šå°è¤‡é›œè¡¨æ ¼é€²è¡Œåœ–åƒè­˜åˆ¥"
    ]
    
    for suggestion in suggestions:
        print(f"   {suggestion}")
    
    print(f"\nğŸ”§ å»ºè­°åŸ·è¡Œé †åºï¼š")
    print(f"   1. å…ˆé‹è¡Œè¨ºæ–·å·¥å…·æŸ¥çœ‹å…·é«”å•é¡Œ")
    print(f"   2. æ ¹æ“šè¨ºæ–·çµæœé‡å°æ€§æ”¹é€²PDFè§£æå™¨")
    print(f"   3. é‡æ–°è§£æä¸¦æª¢æŸ¥å“è³ª")
    print(f"   4. æœ€å¾Œé‡æ–°ç”Ÿæˆåˆ†æå ±å‘Š")

if __name__ == "__main__":
    diagnose_pdf_extraction_quality()
    generate_improvement_suggestions()